{
  "name": "Spark IPython Notebooks",
  "tagline": "Spark & Python (pySpark) tutorials as IPython/Jupyter notebooks",
  "body": "# Spark Python Notebooks  \r\n\r\n[![Join the chat at https://gitter.im/jadianes/spark-py-notebooks](https://badges.gitter.im/Join%20Chat.svg)](https://gitter.im/jadianes/spark-py-notebooks?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge&utm_content=badge)\r\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=EJ54869W5H3KJ)  \r\n\r\nThis is a collection of [IPython notebook](http://ipython.org/notebook.html)/[Jupyter](https://jupyter.org/) \r\nnotebooks intended to train the reader on different [Apache Spark](http://spark.apache.org/) concepts, from \r\nbasic to advanced, by using the **Python** language.  \r\n\r\nIf Python is not your language, and it is R, you may want to have a look at our [R on Apache Spark (SparkR) notebooks](https://github.com/jadianes/spark-r-notebooks) instead. Additionally, if your are interested in being introduced to some basic Data Science\r\nEngineering, you might find [these series of tutorials](https://github.com/jadianes/data-science-your-way)\r\ninteresting. There we explain different concepts and applications \r\nusing Python and R.  \r\n\r\n## Instructions  \r\n\r\nA good way of using these notebooks is by first cloning the repo, and then \r\nstarting your own [IPython notebook](http://ipython.org/notebook.html)/[Jupyter](https://jupyter.org/) in \r\n**pySpark mode**. For example, if we have a *standalone* Spark installation\r\nrunning in our `localhost` with a maximum of 6Gb per node assigned to IPython:  \r\n\r\n    MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"6G\" IPYTHON_OPTS=\"notebook --pylab inline\" ~/spark-1.5.0-bin-hadoop2.6/bin/pyspark\r\n\r\nNotice that the path to the `pyspark` command will depend on your specific \r\ninstallation. So as requirement, you need to have\r\n[Spark installed](https://spark.apache.org/docs/latest/index.html) in \r\nthe same machine you are going to start the `IPython notebook` server.     \r\n\r\nFor more Spark options see [here](https://spark.apache.org/docs/latest/spark-standalone.html). In general it works the rule of passing options\r\ndescribed in the form `spark.executor.memory` as `SPARK_EXECUTOR_MEMORY` when\r\ncalling IPython/pySpark.   \r\n \r\n## Datasets  \r\n\r\nWe will be using datasets from the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html). The results \r\nof this competition can be found [here](http://cseweb.ucsd.edu/~elkan/clresults.html).  \r\n\r\n## References\r\n\r\nThe reference book for these and other Spark related topics is:  \r\n\r\n- *Learning Spark* by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia.  \r\n\r\n## Notebooks  \r\n\r\nThe following notebooks can be examined individually, although there is a more\r\nor less linear 'story' when followed in sequence. By using the same dataset\r\nthey try to solve a related set of tasks with it.  \r\n \r\n### [RDD creation](https://github.com/jadianes/spark-py-notebooks/blob/master/nb1-rdd-creation/nb1-rdd-creation.ipynb)  \r\n\r\nAbout reading files and parallelize.  \r\n  \r\n### [RDDs basics](https://github.com/jadianes/spark-py-notebooks/blob/master/nb2-rdd-basics/nb2-rdd-basics.ipynb)\r\n\r\nA look at `map`, `filter`, and `collect`.  \r\n  \r\n### [Sampling RDDs](https://github.com/jadianes/spark-py-notebooks/blob/master/nb3-rdd-sampling/nb3-rdd-sampling.ipynb)  \r\n\r\nRDD sampling methods explained.    \r\n  \r\n### [RDD set operations](https://github.com/jadianes/spark-py-notebooks/blob/master/nb4-rdd-set/nb4-rdd-set.ipynb)    \r\n\r\nBrief introduction to some of the RDD pseudo-set operations.  \r\n\r\n### [Data aggregations on RDDs](https://github.com/jadianes/spark-py-notebooks/blob/master/nb5-rdd-aggregations/nb5-rdd-aggregations.ipynb)  \r\n\r\nRDD actions `reduce`, `fold`, and `aggregate`.   \r\n\r\n### [Working with key/value pair RDDs](https://github.com/jadianes/spark-py-notebooks/blob/master/nb6-rdd-key-value/nb6-rdd-key-value.ipynb)    \r\n\r\nHow to deal with key/value pairs in order to aggregate and explore data.  \r\n  \r\n### [MLlib: Basic Statistics and Exploratory Data Analysis](https://github.com/jadianes/spark-py-notebooks/blob/master/nb7-mllib-statistics/nb7-mllib-statistics.ipynb)    \r\n\r\nA notebook introducing Local Vector types, basic statistics \r\nin MLlib for Exploratory Data Analysis and model selection.  \r\n  \r\n### [MLlib: Logistic Regression](https://github.com/jadianes/spark-py-notebooks/blob/master/nb8-mllib-logit/nb8-mllib-logit.ipynb)     \r\n\r\nLabeled points and Logistic Regression classification of network attacks in MLlib.\r\nApplication of model selection techniques using correlation matrix and Hypothesis Testing.    \r\n\r\n### [MLlib: Decision Trees](https://github.com/jadianes/spark-py-notebooks/blob/master/nb9-mllib-trees/nb9-mllib-trees.ipynb)  \r\n\r\nUse of tree-based methods and how they help explaining models and\r\n feature selection.  \r\n\r\n### [Spark SQL: structured processing for Data Analysis](https://github.com/jadianes/spark-py-notebooks/blob/master/nb10-sql-dataframes/nb10-sql-dataframes.ipynb)  \r\n\r\nIn this notebook a schema is inferred for our network interactions dataset. Based on that, we use\r\nSpark's SQL `DataFrame` abstraction to perform a more structured exploratory data analysis.  \r\n\r\n\r\n## Applications  \r\n\r\nBeyond the basics. Close to real-world applications using Spark and other technologies.  \r\n\r\n### [Olssen: On-line Spectral Search ENgine for proteomics](https://github.com/jadianes/olssen)  \r\n\r\nSame tech stack this time with an AngularJS client app.  \r\n\r\n### [An on-line movie recommendation web service](https://github.com/jadianes/spark-movie-lens)  \r\n\r\nThis tutorial can be used independently to build a movie recommender model based on the MovieLens dataset. Most of the code in the first part, about how to use ALS with the public MovieLens dataset, comes from my solution to one of the exercises proposed in the [CS100.1x Introduction to Big Data with Apache Spark by Anthony D. Joseph on edX](https://www.edx.org/course/introduction-big-data-apache-spark-uc-berkeleyx-cs100-1x), that is also [**publicly available since 2014 at Spark Summit**](https://databricks-training.s3.amazonaws.com/movie-recommendation-with-mllib.html). \r\n\r\nThere I've added with minor modifications to use a larger dataset and also code about how to store and reload the model for later use. On top of that we build a Flask web service so the recommender can be use to provide movie recommendations on-line.  \r\n\r\n### [KDD Cup 1999](https://github.com/jadianes/kdd-cup-99-spark)  \r\n\r\nMy try using Spark with this classic dataset and Knowledge Discovery competition.  \r\n\r\n## Contributing\r\n\r\nContributions are welcome!  For bug reports or requests please [submit an issue](https://github.com/jadianes/spark-py-notebooks/issues).\r\n\r\n## Contact  \r\n\r\nFeel free to contact me to discuss any issues, questions, or comments.\r\n\r\n* Twitter: [@ja_dianes](https://twitter.com/ja_dianes)\r\n* GitHub: [jadianes](https://github.com/jadianes)\r\n* LinkedIn: [jadianes](https://www.linkedin.com/in/jadianes)\r\n* Website: [jadianes.me](http://jadianes.me)\r\n\r\n## License\r\n\r\nThis repository contains a variety of content; some developed by Jose A. Dianes, and some from third-parties.  The third-party content is distributed under the license provided by those parties.\r\n\r\nThe content developed by Jose A. Dianes is distributed under the following license:\r\n\r\n    Copyright 2016 Jose A Dianes\r\n\r\n    Licensed under the Apache License, Version 2.0 (the \"License\");\r\n    you may not use this file except in compliance with the License.\r\n    You may obtain a copy of the License at\r\n\r\n       http://www.apache.org/licenses/LICENSE-2.0\r\n\r\n    Unless required by applicable law or agreed to in writing, software\r\n    distributed under the License is distributed on an \"AS IS\" BASIS,\r\n    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n    See the License for the specific language governing permissions and\r\n    limitations under the License.\r\n\r\n## Donate  \r\n\r\nIf you find these tutorials useful then you can help me to keep them updated ;)  \r\n\r\n[![Donate](https://img.shields.io/badge/Donate-PayPal-green.svg)](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=EJ54869W5H3KJ)  \r\n",
  "google": "UA-63898672-1",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}