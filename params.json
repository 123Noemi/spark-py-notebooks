{"name":"Spark IPython Notebooks","tagline":"Spark tutorials as IPython notebooks","body":"# Spark IPython Notebooks  \r\n\r\nThis is a collection of IPython notebooks intended to train the reader\r\non different Spark concepts, from basic to advanced, by using the Python\r\nlanguage.  \r\n\r\n## Instructions  \r\n\r\nA good way of using these notebooks is by first cloning the repo, and then \r\nstarting your own [IPython notebook](http://ipython.org/notebook.html) in \r\n**pySpark mode**. For example, if we have a *standalone* Spark installation\r\nrunning in our `localhost` with a maximum of 6Gb per node assigned to IPython:  \r\n\r\n    MASTER=\"spark://127.0.0.1:7077\" SPARK_EXECUTOR_MEMORY=\"6G\" IPYTHON_OPTS=\"notebook --pylab inline\" ~/spark-1.3.1-bin-hadoop2.6/bin/pyspark\r\n\r\nNotice that the path to the `pyspark` command will depend on your specific \r\ninstallation. So as requirement, you need to have\r\n[Spark installed](https://spark.apache.org/docs/latest/index.html) in \r\nthe same machine you are going to start the `IPython notebook` server.     \r\n\r\nFor more Spark options see [here](https://spark.apache.org/docs/latest/spark-standalone.html). In general it works the rule of passign options \r\ndescribed in the form `spark.executor.memory` as `SPARK_EXECUTOR_MEMORY` when\r\ncalling IPython/pySpark.   \r\n \r\n## Datasets  \r\n\r\nWe will be using datasets from the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html).\r\n\r\n## References\r\n\r\nThe reference book for these and other Spark related topics is:  \r\n\r\n- *Learning Spark* by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia.  \r\n\r\n## Notebooks  \r\n\r\nThe following notebooks can be examined individually, although there is a more\r\nor less linear 'story' when followed in sequence. By using the same dataset\r\nthey try to solve a related set of tasks with it.  \r\n \r\n### [RDD creation](https://github.com/jadianes/spark-py-notebooks/blob/master/nb1-rdd-creation/nb1-rdd-creation.ipynb)  \r\n\r\nAbout reading files and parallelize.  \r\n  \r\n### [RDDs basics](https://github.com/jadianes/spark-py-notebooks/blob/master/nb2-rdd-basics/nb2-rdd-basics.ipynb)\r\n\r\nA look at `map`, `filter`, and `collect`.  \r\n  \r\n### [Sampling RDDs](https://github.com/jadianes/spark-py-notebooks/blob/master/nb3-rdd-sampling/nb3-rdd-sampling.ipynb)  \r\n\r\nRDD sampling methods explained.    \r\n  \r\n### [RDD set operations](https://github.com/jadianes/spark-py-notebooks/blob/master/nb4-rdd-set/nb4-rdd-set.ipynb)    \r\n\r\nBrief introduction to some of the RDD pseudo-set operations.  \r\n\r\n### [Data aggregations on RDDs](https://github.com/jadianes/spark-py-notebooks/blob/master/nb5-rdd-aggregations/nb5-rdd-aggregations.ipynb)  \r\n\r\nRDD actions `reduce`, `fold`, and `aggregate`.   \r\n\r\n### [Working with key/value pair RDDs](https://github.com/jadianes/spark-py-notebooks/blob/master/nb6-rdd-key-value/nb6-rdd-key-value.ipynb)    \r\n\r\nHow to deal with key/value pairs in order to aggregate and explore data.  \r\n  \r\n### [MLlib: Basic Statistics and Exploratory Data Analysis](https://github.com/jadianes/spark-py-notebooks/blob/master/nb7-mllib-statistics/nb7-mllib-statistics.ipynb)    \r\n\r\nA notebook introducing Local Vector types, basic statistics \r\nin MLlib for Exploratory Data Analysis and model selection.  \r\n  \r\n### [MLlib: Logistic Regression](https://github.com/jadianes/spark-py-notebooks/blob/master/nb8-mllib-logit/nb8-mllib-logit.ipynb)     \r\n\r\nLabeled points and Logistic Regression classification of network attacks in MLlib.\r\nApplication of model selection techniques using correlation matrix and Hypothesis Testing.    \r\n\r\n### [MLlib: Decision Trees](https://github.com/jadianes/spark-py-notebooks/blob/master/nb9-mllib-trees/nb9-mllib-trees.ipynb)  \r\n\r\nUse of three-based methods and how they help explaining models and\r\n feature selection.  \r\n\r\n### [Spark SQL: structured processing for Data Analysis](https://github.com/jadianes/spark-py-notebooks/blob/master/nb10-sql-dataframes/nb10-sql-dataframes.ipynb)  \r\n\r\nIn this notebook a schema is inferred for our network interactions dataset. Based on that, we use\r\nSpark's SQL `DataFrame` abstraction to perform a more structured exploratory data analysis.  \r\n\r\n### MLlib: Clustering  \r\n\r\nK-means clustering for exploratory data analysis and network attack detection.  \r\n\r\n### Streaming: basics  \r\n\r\nBasics of Spark streaming API.  \r\n\r\n### Streaming: usage with MLlib  \r\n\r\nUsage of Spark streaming API together with MLlib (e.g. incremental data clustering).  \r\n\r\n\r\n## Applications  \r\n\r\nBeyond the basics. Close to real-world applications using Spark and other technologies.  \r\n\r\n### [An on-line movie recommendation service using Spark & Flask](https://github.com/jadianes/spark-py-notebooks/tree/master/movie-lens-recommender)  \r\n\r\nA two part tutorial + Python scripts to run your own scalable movie recommendation service using Spark and Flask.  \r\n","google":"UA-63898672-1","note":"Don't delete this file! It's used internally to help with page regeneration."}