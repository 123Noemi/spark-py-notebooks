{
 "metadata": {
  "name": "",
  "signature": "sha256:61ef237ec10ae4548c85ec209027d90d83c230e70b48b6cfa93fecd47115d9b2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "MLlib: Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "[Introduction to Spark with Python, by Jose A. Dianes](http://jadianes.github.io/spark-py-notebooks)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this notebook we will use Spark's machine learning library [MLlib](https://spark.apache.org/docs/latest/mllib-guide.html) to perform **K-means Clustering** over our network attacks datasets. We will use the complete [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) datasets in order to test Spark capabilities with large datasets. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Once we have our data clustered, we will use our knowledge of Spark SQL and data frames from previous notebooks in order to explore each of the obtained clusters in terms of its features.  "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally we will see how a set of clusters can be used to detect network attacks. We will use our test data as new incoming data. We will assign each interaction to a cluster and, based on the most frequent tag on the cluster, we will decide if the new data is or not an attack. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "At the time of processing this notebook, our Spark cluster contains:  \n",
      "\n",
      "- Eight nodes, with one of them acting as master and the rest as workers.  \n",
      "- Each node contains 8Gb of RAM, with 6Gb being used for each node.  \n",
      "- Each node has a 2.4Ghz Intel dual core processor.  \n",
      "- Running Apache Spark 1.3.1.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting the data and creating the RDD"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "As we said, this time we will use the complete dataset provided for the [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html), containing nearly half million nework interactions. The file is provided as a Gzip file that we will download locally. Remember that the file must be accessible to every worker in our Spark cluster (in our case we use [NFS](https://en.wikipedia.org/wiki/Network_File_System)).    "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "f = urllib.urlretrieve (\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\", \"kddcup.data.gz\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_file = \"./kddcup.data.gz\"\n",
      "raw_data = sc.textFile(data_file)\n",
      "\n",
      "print \"Train data size is {}\".format(raw_data.count())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Train data size is 4898431\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The [KDD Cup 1999](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) also provide test data that we will load in a separate RDD.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ft = urllib.urlretrieve(\"http://kdd.ics.uci.edu/databases/kddcup99/corrected.gz\", \"corrected.gz\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "test_data_file = \"./corrected.gz\"\n",
      "test_raw_data = sc.textFile(test_data_file)\n",
      "\n",
      "print \"Test data size is {}\".format(test_raw_data.count())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test data size is 311029\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 4,
     "metadata": {},
     "source": [
      "Unsupervised learning with K-Means Clustering"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Firs of all, we prepare the data for clustering input. The data contains non-numeric features, and we want to exclude them since k-means works just with numeric features. These are the first three and the last column in each data row that is the label. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In order to do that, we define a function that we apply to the *RDD* as a `Spark` **transformation** by using `map`. The **action** that actually retrieves the data is `values`. Remember that we can apply as many transofmrations as we want without making `Spark` start any processing. Is is when we trigger an action when all the transformations are applied.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def parse_interaction(line):\n",
      "    \"\"\"\n",
      "    Parses a network data interaction.\n",
      "    \"\"\"\n",
      "    line_split = line.split(\",\")\n",
      "    clean_line_split = [line_split[0]]+line_split[4:-1]\n",
      "    return (line_split[-1], array([float(x) for x in clean_line_split]))\n",
      "\n",
      "parsed_data = raw_data.map(parse_interaction)\n",
      "parsed_data_values = parsed_data.values().cache()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will also standardise our data as we have done so far when performing distance-based clustering."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from time import time\n",
      "from pyspark.mllib.feature import StandardScaler\n",
      "standardizer = StandardScaler(True, True)\n",
      "\n",
      "t0 = time()\n",
      "standardizer_model = standardizer.fit(parsed_data_values)\n",
      "tt = time() - t0\n",
      "\n",
      "standardized_data_values = standardizer_model.transform(parsed_data_values)\n",
      "print \"Data standardized in {} seconds\".format(round(tt,3))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Data standardized in 310.946 seconds\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And now we are ready to perform k-means clustering. The call to `KMeans.train` triggers the clustering process and returns a list of cluster centroids as well as a model que can use to assign a point to a cluster by calling `predict`. This can be seen in the `error` function declared bellow. There are other parameters for the `train` function, that are explained in the [Spark reference](http://spark.apache.org/docs/latest/mllib-clustering.html#k-means). In our case we will set three of the, the maximum number of iterations for the algorithms, the number of runs, and how do we want to initialise the cluster centroids for the iterative clustering process. "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We also need to determine a good value for K (the number of clusters for k-means). We will do this visually. We will try with a range of values from 5 to 100, and plot their *Within Set Sum of Squared Errors* (the square root of the sum of squared ditances for each point in a cluster to its centroid) as follows. This process will take a long while. Each time a new value of K has been tried you will see some output printed out. If you want to skip the whole process, you can change the list of values in the for loop to just the best value we have found after this block."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from pyspark.mllib.clustering import KMeans\n",
      "\n",
      "clusters = {}\n",
      "WSSSE = {}\n",
      "\n",
      "# Evaluate clustering by computing Within Set Sum of Squared Errors\n",
      "def error(point, clusters_broadcast):\n",
      "    center = clusters_broadcast.value.centers[clusters_broadcast.value.predict(point)]\n",
      "    return sqrt(sum([x**2 for x in (point - center)]))\n",
      "\n",
      "    \n",
      "for k_value in [5, 10, 25, 50, 80, 100]:    \n",
      "    t0 = time()\n",
      "    clusters[k_value] = KMeans.train(\n",
      "            standardized_data_values, \n",
      "            k_value, \n",
      "            maxIterations=10, \n",
      "            runs=4, \n",
      "            initializationMode=\"random\"\n",
      "    )\n",
      "    tt = time() - t0\n",
      "\n",
      "    # we need to broadcast this variable if we want to use it in a\n",
      "    # Spark transformation (like the following map)\n",
      "    current_clusters = sc.broadcast(clusters[k_value])\n",
      "    \n",
      "    WSSSE[k_value] = standardized_data_values.map(lambda point: error(point, current_clusters)).reduce(lambda x, y: x + y)\n",
      "    print \"K={}, WSSSE={}, clustered in {} seconds\".format(k_value, WSSSE[k_value], round(tt,3))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we build a [Pandas data frame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) that we will use to plot the WSSSE values."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "\n",
      "kmeans_results = pd.DataFrame(WSSE)\n",
      "\n",
      "kmeans_results.plot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "It seems that after `k=80` the WSSSE value doesn't decrease at the same rate. We will use that k value for our further explorations.  "
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Visualising clusters with the help of PCA"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In this section we want to do two things. First we want to visualise the clusters in a two dimensional space. By doing so we will get a feeling of cluster sizes and proximity. The second thing we want to do is to find out the main discriminant variables in our dataset. [Principal Component Analysis](https://en.wikipedia.org/wiki/Principal_component_analysis) will help to get both things done."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Explaining major clusters"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will end up this introduction to clustering with Spark by having a look at tag composition and feature values for the five bigger clusters.  "
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}