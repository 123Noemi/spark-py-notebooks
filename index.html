<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Spark IPython Notebooks by jadianes</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/github-light.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Spark IPython Notebooks</h1>
        <p>Spark tutorials as IPython notebooks</p>

        <p class="view"><a href="https://github.com/jadianes/spark-py-notebooks">View the Project on GitHub <small>jadianes/spark-py-notebooks</small></a></p>


        <ul>
          <li><a href="https://github.com/jadianes/spark-py-notebooks/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/jadianes/spark-py-notebooks/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/jadianes/spark-py-notebooks">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <h1>
<a id="spark-ipython-notebooks" class="anchor" href="#spark-ipython-notebooks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Spark IPython Notebooks</h1>

<p>This is a collection of IPython notebooks intended to train the reader
on different Spark concepts, from basic to advanced, by using the Python
language.  </p>

<h2>
<a id="instructions" class="anchor" href="#instructions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Instructions</h2>

<p>A good way of using these notebooks is by first cloning the repo, and then 
starting your own <a href="http://ipython.org/notebook.html">IPython notebook</a> in 
<strong>pySpark mode</strong>. For example, if we have a <em>standalone</em> Spark installation
running in our <code>localhost</code> with a maximum of 6Gb per node assigned to IPython:  </p>

<pre><code>MASTER="spark://127.0.0.1:7077" SPARK_EXECUTOR_MEMORY="6G" IPYTHON_OPTS="notebook --pylab inline" ~/spark-1.2.1-bin-hadoop2.4/bin/pyspark
</code></pre>

<p>Notice that the path to the <code>pyspark</code> command will depend on your specific 
installation. So as requirement, you need to have
<a href="https://spark.apache.org/docs/latest/index.html">Spark installed</a> in 
the same machine you are going to start the <code>IPython notebook</code> server.     </p>

<p>For more Spark options see <a href="https://spark.apache.org/docs/latest/spark-standalone.html">here</a>. In general it works the rule of passign options 
described in the form <code>spark.executor.memory</code> as <code>SPARK_EXECUTOR_MEMORY</code> when
calling IPython/pySpark.   </p>

<h2>
<a id="datasets" class="anchor" href="#datasets" aria-hidden="true"><span class="octicon octicon-link"></span></a>Datasets</h2>

<p>We will be using datasets from the <a href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html">KDD Cup 1999</a>.</p>

<h2>
<a id="references" class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h2>

<p>The reference book for these and other Spark related topics is:  </p>

<ul>
<li>
<em>Learning Spark</em> by Holden Karau, Andy Konwinski, Patrick Wendell, and Matei Zaharia.<br>
</li>
</ul>

<h2>
<a id="notebooks" class="anchor" href="#notebooks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notebooks</h2>

<p>The following notebooks can be examined individually, although there is a more
or less linear 'story' when followed in sequence. By using the same dataset
they try to solve a related set of tasks with it.  </p>

<h3>
<a id="rdd-creation" class="anchor" href="#rdd-creation" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb1-rdd-creation/nb1-rdd-creation.ipynb">RDD creation</a>
</h3>

<p>About reading files and parallelize.  </p>

<h3>
<a id="rdds-basics" class="anchor" href="#rdds-basics" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb2-rdd-basics/nb2-rdd-basics.ipynb">RDDs basics</a>
</h3>

<p>A look at <code>map</code>, <code>filter</code>, and <code>collect</code>.  </p>

<h3>
<a id="sampling-rdds" class="anchor" href="#sampling-rdds" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb3-rdd-sampling/nb3-rdd-sampling.ipynb">Sampling RDDs</a>
</h3>

<p>RDD sampling methods explained.    </p>

<h3>
<a id="rdd-set-operations" class="anchor" href="#rdd-set-operations" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb4-rdd-set/nb4-rdd-set.ipynb">RDD set operations</a>
</h3>

<p>Brief introduction to some of the RDD pseudo-set operations.  </p>

<h3>
<a id="data-aggregations-on-rdds" class="anchor" href="#data-aggregations-on-rdds" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb5-rdd-aggregations/nb5-rdd-aggregations.ipynb">Data aggregations on RDDs</a>
</h3>

<p>RDD actions <code>reduce</code>, <code>fold</code>, and <code>aggregate</code>.   </p>

<h3>
<a id="working-with-keyvalue-pair-rdds" class="anchor" href="#working-with-keyvalue-pair-rdds" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb6-rdd-key-value/nb6-rdd-key-value.ipynb">Working with key/value pair RDDs</a>
</h3>

<p>How to deal with key/value pairs in order to aggregate and explore data.  </p>

<h3>
<a id="mllib-basic-statistics-and-exploratory-data-analysis" class="anchor" href="#mllib-basic-statistics-and-exploratory-data-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb7-mllib-statistics/nb7-mllib-statistics.ipynb">MLlib: Basic Statistics and Exploratory Data Analysis</a>
</h3>

<p>A notebook introducing Local Vector types, basic statistics 
in MLlib for Exploratory Data Analysis and model selection.  </p>

<h3>
<a id="mllib-logistic-regression" class="anchor" href="#mllib-logistic-regression" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb8-mllib-logit/nb8-mllib-logit.ipynb">MLlib: Logistic Regression</a>
</h3>

<p>Labeled points and Logistic Regression classification of network attacks in MLlib.
Application of model selection techniques using correlation matrix and Hypothesis Testing.    </p>

<h3>
<a id="mllib-decision-trees" class="anchor" href="#mllib-decision-trees" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb9-mllib-trees/nb9-mllib-trees.ipynb">MLlib: Decision Trees</a>
</h3>

<p>Use of three-based methods and how they help explaining models and
 feature selection.  </p>

<h3>
<a id="spark-sql-structured-processing-for-data-analysis" class="anchor" href="#spark-sql-structured-processing-for-data-analysis" aria-hidden="true"><span class="octicon octicon-link"></span></a><a href="https://github.com/jadianes/spark-py-notebooks/blob/master/nb10-sql-dataframes/nb10-sql-dataframes.ipynb">Spark SQL: structured processing for Data Analysis</a>
</h3>

<p>In this notebook a schema is inferred for our network interactions dataset. Based on that, we use
Spark's SQL <code>DataFrame</code> abstraction to perform a more structured exploratory data analysis.  </p>

<h3>
<a id="mllib-clustering-and-anomality-detection" class="anchor" href="#mllib-clustering-and-anomality-detection" aria-hidden="true"><span class="octicon octicon-link"></span></a>MLlib: Clustering and anomality detection</h3>

<p>K-means clustering for anomality detection.  </p>

<h3>
<a id="streaming-basics" class="anchor" href="#streaming-basics" aria-hidden="true"><span class="octicon octicon-link"></span></a>Streaming: basics</h3>

<p>Basics of Spark streaming API.  </p>

<h3>
<a id="streaming-usage-with-mllib" class="anchor" href="#streaming-usage-with-mllib" aria-hidden="true"><span class="octicon octicon-link"></span></a>Streaming: usage with MLlib</h3>

<p>Usage of Spark streaming API together with MLlib (e.g. incremental data clustering).  </p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/jadianes">jadianes</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
              <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-63898672-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
